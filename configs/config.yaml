project: learning-agency-lab-pil-data-detection
name: experiment-tracking-Learning-Agency-Lab-PII-Data-Detection
wandb:
  WANDB_API_KEY: WANDB_API_KEY
  entity: baolocpham
  project: learning-agency-lab-pil-data-detection
  name: learning-agency-lab-pil-data-detection
  group: learning-agency-lab-pil-data-detection_training
  job_type: Train_Supervised
parameters:
  root_data_dir: "./data"
  n_fold: 4
  save_model_dir: "./outputs"
  tmp_dir: "./tmp"
  preprocess_text: False
  target: [
    "B-EMAIL", "B-ID_NUM", "B-NAME_STUDENT", "B-PHONE_NUM", 
    "B-STREET_ADDRESS", "B-URL_PERSONAL", "B-USERNAME", "I-ID_NUM", 
    "I-NAME_STUDENT", "I-PHONE_NUM", "I-STREET_ADDRESS", "I-URL_PERSONAL"]
  num_proc: 3
  debug: False
  
  train_stage_1:
    wandb: False
    select: "base"
    base_model_name: "microsoft/deberta-v3-{select}"
    model_path: ""
    output_dir: "./"
    test_size: 0.2
    max_len: 512
    freeze_embeddings: False
    freeze_n_layers: 6
    seed: 42
    fp16: True
    learning_rate: 2e-5
    num_train_epochs: 3
    per_device_train_batch_size: 4,
    per_device_eval_batch_size: 4
    gradient_accumulation_steps: 2
    report_to: "none"
    do_eval: True
    evaluation_strategy: "epoch"
    save_strategy: "epoch"
    save_total_limit: 1
    overwrite_output_dir: True
    load_best_model_at_end: True
    lr_scheduler_type: 'cosine'
    metric_for_best_model: "f1"
    greater_is_better: True
    warmup_ratio: 0.1
    weight_decay: 0.01
    logging_steps: 20


  inference_stage_1:
    is_ensemble: False
    input_prepared_dir: "./"
    input_prepared_file: "test.csv"
    select: "base"
    model_name: "microsoft/deberta-v3-{select}"
    only_model_name: "deberta-v3-{select}"
    n_fold: 4
    fold_to_inference: 0
    batch_size: 32
    freezing: False
    max_len: 32
    pooling: "GemText"
    load_model_dir: ""
    output_dir: ""
    have_next_stage: False
    output_file: "stage_1_output.csv"